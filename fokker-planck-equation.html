<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Mathematical Building Blocks of Diffusion Generative Models - Tonghe Zhang</title>
    <link rel="icon" type="image/png" href="favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/svg+xml" href="favicon/favicon.svg" />
    <link rel="shortcut icon" href="favicon/favicon.ico" />
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-title" content="TongheZhang" />
    <link rel="manifest" href="/favicon/site.webmanifest" />
    
    <!-- LaTeX-style fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Computer+Modern+Serif:wght@400;700&family=Computer+Modern+Sans:wght@400;700&family=Computer+Modern+Typewriter:wght@400&display=swap" rel="stylesheet">
    
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Computer Modern Serif', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            background-color: #ffffff;
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px 20px;
            font-size: 1.1em;
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 2px solid #2c3e50;
        }

        .header h1 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 2.2em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
            line-height: 1.2;
        }

        .header .subtitle {
            font-size: 1.3em;
            color: #34495e;
            margin-bottom: 20px;
            font-weight: 400;
        }

        .header .author {
            font-size: 1.1em;
            color: #666;
            margin-bottom: 10px;
        }

        .header .date {
            font-size: 1em;
            color: #888;
            font-style: italic;
        }

        .back-link {
            display: inline-block;
            margin-bottom: 30px;
            font-size: 1.1em;
            color: #2980b9;
            text-decoration: none;
            font-weight: 500;
        }

        .back-link:hover {
            color: #3498db;
            text-decoration: underline;
        }

        .abstract {
            background-color: #f8f9fa;
            border-left: 4px solid #2980b9;
            padding: 20px;
            margin-bottom: 40px;
            border-radius: 0 8px 8px 0;
        }

        .abstract h2 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .abstract p {
            font-size: 1.1em;
            line-height: 1.7;
            color: #444;
            text-align: justify;
        }

        .toc {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 40px;
        }

        .toc h2 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 1.3em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .toc ul {
            list-style-type: none;
            padding-left: 0;
        }

        .toc li {
            margin-bottom: 8px;
        }

        .toc a {
            color: #2980b9;
            text-decoration: none;
            font-weight: 500;
        }

        .toc a:hover {
            color: #3498db;
            text-decoration: underline;
        }

        .section {
            margin-bottom: 50px;
        }

        .section h2 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 1.8em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 20px;
            border-bottom: 2px solid #34495e;
            padding-bottom: 10px;
        }

        .section h3 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 1.4em;
            font-weight: bold;
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        .section h4 {
            font-family: 'Computer Modern Sans', sans-serif;
            font-size: 1.2em;
            font-weight: bold;
            color: #2c3e50;
            margin-top: 25px;
            margin-bottom: 12px;
        }

        .section p {
            margin-bottom: 15px;
            text-align: justify;
            line-height: 1.7;
        }

        .section ul, .section ol {
            margin-bottom: 15px;
            padding-left: 25px;
        }

        .section li {
            margin-bottom: 8px;
            line-height: 1.6;
        }

        /* Math styling - keep LaTeX style, no italics */
        .math {
            font-family: 'Computer Modern Serif', 'Times New Roman', serif;
            font-style: normal;
        }

        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 8px;
            border: 1px solid #e9ecef;
        }

        .equation-number {
            float: right;
            font-weight: bold;
            color: #666;
        }

        /* Theorem and proof styling - darker blues */
        .theorem, .corollary, .definition, .proof {
            margin: 20px 0;
            padding: 15px;
            border-radius: 8px;
        }

        .theorem {
            background-color: #e3f2fd;
            border-left: 4px solid #1976d2;
        }

        .corollary {
            background-color: #e8f5e8;
            border-left: 4px solid #2e7d32;
        }

        .definition {
            background-color: #fff8e1;
            border-left: 4px solid #f57c00;
        }

        .proof {
            background-color: #f5f5f5;
            border-left: 4px solid #424242;
        }

        .theorem h3, .corollary h3, .definition h3, .proof h3 {
            margin-top: 0;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        /* Code and technical terms */
        code {
            font-family: 'Computer Modern Typewriter', 'Courier New', monospace;
            background-color: #f1f3f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-size: 0.9em;
        }

        /* Links - darker blue to match index.html */
        a {
            color: #1976d2;
            text-decoration: none;
            font-weight: 500;
        }

        a:hover {
            color: #1565c0;
            text-decoration: underline;
        }

        /* Responsive design */
        @media (max-width: 768px) {
            body {
                padding: 20px 15px;
                font-size: 1.0em;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .header .subtitle {
                font-size: 1.1em;
            }
            
            .section h2 {
                font-size: 1.5em;
            }
            
            .section h3 {
                font-size: 1.2em;
            }
        }

        /* Special formatting for mathematical content - no italics */
        .math-inline {
            font-style: normal;
        }

        .bold {
            font-weight: bold;
        }

        .italic {
            font-style: italic;
        }

        .underline {
            text-decoration: underline;
        }

        /* Highlight important text */
        .highlight {
            background-color: #fff3cd;
            padding: 2px 4px;
            border-radius: 3px;
        }

        /* Fix for vector notation - ensure proper display */
        .vector {
            font-family: 'Computer Modern Serif', 'Times New Roman', serif;
            font-style: normal;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <a href="blogs.html" class="back-link">&larr; Back to Blogs</a>
    
    <div class="header">
        <h1>The Mathematical Building Blocks of<br>Diffusion Generative Models</h1>
        <div class="subtitle">Understanding the Fokker-Planck Equation and Its Applications</div>
        <div class="author">Tonghe Zhang</div>
        <div class="date">October, 2025</div>
    </div>

    <div class="abstract">
        <h2>Abstract</h2>
        <p>We provide a proof of the necessary condition of the Fokker-Planck (FP) equation and derive some important conclusions, including the continuity equation and the reverse-time diffusion process. Then we study a typical variant of diffusion process, namely, flow matching processes with linear interpolation paths, and study the relationship between its score and velocity. Finally, we study how to adopt the FP equation to derive an ODE-SDE conversion formula that links flow ODEs with diffusion SDEs.</p>
    </div>

    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#preliminaries">1. Preliminaries</a></li>
            <li><a href="#fokker-planck">2. Fokker-Planck Equations</a></li>
            <li><a href="#special-case-ode">3. Special Case I: ODE, Continuity Equation, and Flow-matching</a></li>
            <li><a href="#special-case-diffusion">4. Special Case II: Forward and Reverse-time Diffusion Process</a></li>
            <li><a href="#discussion">5. Discussion: Connecting Flows with Diffusion</a></li>
        </ul>
    </div>

    <section class="section" id="preliminaries">
        <h2>1. Preliminaries</h2>
        <p>To fully understand the proof of FP equations, we need to recall several results from probability, analysis and linear algebra.</p>
        
        <h3>1.1 Wiener Process</h3>
        <p>Recall that a Wiener process $W_t$ in $\mathbb{R}^d$ possesses the following property:</p>
        
        <div class="equation">
            $$\forall t, h \in \mathbb{R}_+: \quad W_{t+h} - W_t \sim \mathcal{N}(0, h\mathbb{I}_{d\times d})$$
        </div>
        
        <p>This naturally implies that, for any matrix $A \in \mathbb{R}^{d\times d}$ independent of $W_t$,</p>
        
        <div class="equation">
            $$\mathbb{E}[W_{t+h} - W_t] = 0$$
            $$\mathbb{E}[(W_{t+h} - W_t)^\top A (W_{t+h} - W_t)] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,h\mathbb{I}_{d\times d})}[\epsilon^\top A \epsilon]$$
        </div>
        
        <p>Here, $\epsilon \in \mathbb{R}^d$, which has independent coordinates. The expectations are taken with respect to the randomness in $W_t$.</p>

        <h3>1.2 Hutchinson's Trace Estimator</h3>
        <p>For $\epsilon \sim \mathcal{N}(0, h\mathbb{I}_{d\times d})$ and matrix $A \in \mathbb{R}^{d\times d}$, the trace of $A$ is the expected quadratic form of $\epsilon$ and $A$.</p>
        
        <div class="equation">
            $$\mathbb{E}_\epsilon[\epsilon^\top A \epsilon] = \mathbb{E}_{\epsilon_1,\epsilon_2,\ldots,\epsilon_d}\left[\sum_{i,j} \epsilon_i A_{i,j} \epsilon_j\right] = \sum_{i,j} A_{i,j} \mathbb{E}_{\epsilon_1,\epsilon_2,\ldots,\epsilon_d}[\epsilon_i \epsilon_j] = \sum_{i,j} A_{i,j} \delta_{i,j} = \text{tr} A$$
        </div>
        
        <p>Consequently, $\hat{A}_\epsilon = \epsilon^\top A \epsilon$ is an unbiased estimator of $\text{tr} A$.</p>

        <h3>1.3 Hessian Matrix and Laplacian</h3>
        <p>For a scalar functional $u: \mathbb{R}^d \to \mathbb{R}$ (which usually takes the physical meaning of a potential field over $\mathbb{R}^d$), the Hessian matrix is defined by packing the second-order derivatives into a matrix:</p>
        
        <div class="equation">
            $$H_u = \nabla^2 u = \left(\frac{\partial^2}{\partial x_i \partial x_j} u(\vec{x})\right)_{i,j \in [d]^2} \in \mathbb{R}^{d \times d}$$
        </div>
        
        <p>The Laplacian of $u$ is defined as the sum of second order derivatives at the same coordinates:</p>
        
        <div class="equation">
            $$\Delta u = \nabla \cdot \nabla u = \sum_{i=1}^d \frac{\partial}{\partial x_i} u(\vec{x}) \in \mathbb{R}$$
        </div>
        
        <p>By definition, the trace of the Hessian matrix is the Laplacian.</p>
        
        <div class="equation">
            $$\text{tr} \nabla^2 u = \Delta u$$
        </div>
        
        <div class="corollary">
            <h3>Corollary</h3>
            <p>For a scalar functional $u: \mathbb{R}^d \to \mathbb{R}$ and white Gaussian vector $\epsilon \sim \mathcal{N}(0, h\mathbb{I}_{d\times d})$, the expected value of the quadratic form obtained from the difference of Wiener process $W_t$ and Hessian matrix $\nabla^2 u$ is equivalent to the Laplacian of the potential field $u$.</p>
            
            <div class="equation">
                $$\mathbb{E}[(W_{t+h} - W_t)^\top \nabla^2 u (W_{t+h} - W_t)] = \Delta u$$
            </div>
        </div>

        <h3>1.4 The Laplacian of probability density; score function</h3>
        <p>The Laplacian of a probability density function $p$ often appears in the literature of diffusion models. By definition, we have</p>
        
        <div class="equation">
            $$\nabla^2 p = \nabla \cdot (\nabla p)$$
        </div>
        
        <p>Furthermore, we relate the density gradient to the score function, $s(x) = \nabla \log p(x) = \frac{\nabla p(x)}{p(x)}$, which implies</p>
        
        <div class="equation">
            $$\nabla^2 p = \nabla \cdot (p \nabla \log p)$$
        </div>
        
        <div class="theorem">
            <h3>Theorem</h3>
            <p>The Laplacian of a probability density is the divergence of the product between density and its score function:</p>
            
            <div class="equation">
                $$\nabla^2 p = \nabla \cdot (p(x) s(x))$$
            </div>
        </div>

        <h3>1.5 Point-wise Equivalence, Test Functions, and Integration by Parts</h3>
        <p>To prove that two real functionals are point-wise equal, we can resort to evaluating the inner products of these functions with the same test function. If the test results (i.e. the inner products) are always the same for any test function used, then the functions are the same. Rigorously speaking,</p>
        
        <div class="theorem">
            <h3>Theorem</h3>
            <p>For arbitrary integrable functions $g_1, g_2: \mathbb{R}^d \to \mathbb{R}$ it holds that</p>
            
            <div class="equation">
                $$g_1(x) = g_2(x) \text{ for all } x \in \mathbb{R}^d \quad \Leftrightarrow \quad \int f(x) g_1(x) dx = \int f(x) g_2(x) dx \text{ for all test functions } f$$
            </div>
        </div>
        
        <p>Recall that a test function is an infinitely differentiable function with and non-zero on a compact support, and it includes dirac delta functions. So, from RHS to LHS is simple, just pick the dirac delta. The transition from LHS to RHS is also straightforward.</p>
        
        <h4>More properties of test functions</h4>
        <p>First, test functions are zero at infinity, or the border of the compact support, due to definition.</p>
        
        <p>Second, for arbitrary test functions $f_1, f_2$, we can use integration by parts to derive</p>
        
        <div class="equation">
            $$\int_D f_1(x) \frac{\partial}{\partial x_i} f_2(x) dx = f_1(x)f_2(x)\bigg|_{\partial D} - \int f_2(x) \frac{\partial}{\partial x_i} f_1(x) dx = 0 - \int f_2(x) \frac{\partial}{\partial x_i} f_1(x) dx = -\int f_2(x) \frac{\partial}{\partial x_i} f_1(x) dx$$
        </div>
        
        <p>By using this together with the definition of the divergence and Laplacian, we get the identities:</p>
        
        <div class="equation">
            $$\int \nabla^T u(x) \vec{v}(x) dx = -\int u(x) \text{div}(\vec{v})(x) dx \quad (u: \mathbb{R}^d \to \mathbb{R}, \vec{v}: \mathbb{R}^d \to \mathbb{R}^d)$$
            $$\int u(x) \Delta v(x) dx = \int v(x) \Delta u(x) dx \quad (u: \mathbb{R}^d \to \mathbb{R}, v: \mathbb{R}^d \to \mathbb{R})$$
        </div>
        
        <p>Notice that in each equation, $x$ is understood as $(x_1,x_2,\ldots,x_d)$ and</p>
        
        <div class="equation">
            $$\int_\Omega f(x) dx = \int_{\Omega_1\times \ldots \times \Omega_d} f(x_1,x_2,\ldots,x_d) \mathrm{d}x_1 \mathrm{d}x_2 \ldots \mathrm{d}x_d$$
        </div>
        
        <p>We can show the first equation by recursively calling integration by parts on each coordinate $x_i$. The second identity is proved by calling the integration by parts twice.</p>
    </section>

    <section class="section" id="fokker-planck">
        <h2>2. Fokker-Planck Equations</h2>
        <p>For a stochastic process determined by a stochastic differential equation (SDE),</p>
        
        <div class="equation">
            $$\mathrm{d}X_t = \text{some function of } X_t, t, \text{ and random noise}$$
        </div>
        
        <p>the Fokker-Planck (FP) equation tells us how the marginal density of this process $X_t \sim p_t(\cdot)$ changes when the time evolves, and it expresses the marginal density in terms of the coefficients of the underlying SDE. The FP equations precisely characterize the <em>dynamics</em> of this stochastic process, connecting the <em>differential equations</em> of evolution with the <em>statistical</em> properties of this system.</p>
        
        <p>In the following sections, we will detail the proof of the Fokker-Planck equation in its most general form. Afterwards, we will use the FP equation to study two special types of stochastic processes, the first one is deterministic, and the second is stochastic, but the noise is irrelevant to the current state. These two cases are of particular interest to machine learning studies, as the former leads to flow-matching process, and the latter leads to diffusion models.</p>

        <h3>2.1 How an SDE Drives the Evolution of Marginal Density</h3>
        <p>First, we provide a statement of the Fokker-Planck equation.</p>
        
        <div class="theorem">
            <h3>Theorem (Fokker-Planck Equation)</h3>
            <p>Let $p_t$ be a probability path and consider Itô SDE</p>
            
            <div class="equation">
                $$X_0 \sim p_0, \quad \mathrm{d}X_t = \mu(X_t, t) \mathrm{d}t + \sigma(X_t, t) dW_t$$
            </div>
            
            <p>where $X_t \in \mathbb{R}^d$. Then $X_t$ has distribution $p_t$ for all $0 \leq t \leq 1$ if and only if the Fokker-Planck equation holds:</p>
            
            <div class="equation">
                $$\partial_t p_t(x) = -\nabla \cdot (p_t \mu_t)(x) + \frac{1}{2} \Delta(\sigma_t^2 p_t)(x) \quad \text{for all } x \in \mathbb{R}^d, 0 \leq t \leq 1$$
            </div>
            
            <p>where we use the abbreviations $\mu_t(x) = \mu(x_t, t)$ and $\sigma_t^2(x) = \langle \sigma(x_t, t), \sigma(x_t, t) \rangle$.</p>
        </div>

        <div class="proof">
            <h3>Proof</h3>
            <p>We only show the necessary condition, which is nontrivial.</p>
            
            <p>This equation shows point-wise equivalence between two real functionals. To show that, we pick an arbitrary test function $f: \mathbb{R}^d \to \mathbb{R}$, and show that the left and the right are equivalent after applying $f$ to form inner products. We want to show that</p>
            
            <div class="equation">
                $$\int \partial_t p_t(x) f(x) dx = \int \left[-\nabla \cdot (p_t \mu_t)(x) + \frac{1}{2} \Delta(\sigma_t^2 p_t)(x)\right] f(x) dx$$
            </div>
            
            <p>holds for all test functions. And then for any point $x$, we pick $f(z) = \delta(z-x)$ and finish proving that LHS=RHS at any point.</p>
            
            <p>Let us start from LHS. We will use the property that $p_t(x)$ is in fact a probability density, and the test function is time-irrelevant. Consequently, LHS can be written into an expectation:</p>
            
            <div class="equation">
                $$\text{LHS} = \partial_t \mathbb{E}_{x \sim p_t}[f(x)] = \lim_{h\to 0}\frac{1}{h} \mathbb{E}[f(X_{t+h}) - f(X_t)] = \lim_{h\to 0}\frac{1}{h} \mathbb{E}\left[\mathbb{E}[f(X_{t+h}) - f(X_t) | X_t]\right]$$
            </div>
            
            <p>We will use the second-order Taylor expansion to evaluate the enumerator, since this limit eliminates any remaining terms of order $o(h)$. Then we will call the helper functions derived in the preliminary section and take the limit to finish the proof. In what follows, we neglect the higher order terms for better readability.</p>
            
            <div class="equation">
                $$\begin{aligned}
                f(X_{t+h}) - f(X_t) &= f(X_t + h \mu_t(X_t) + \sigma_t(W_{t+h} - W_t)) - f(X_t) \\
                &= \nabla f(X_t)^\top(h \mu_t(X_t) + \sigma_t(W_{t+h} - W_t)) \\
                &\quad + \frac{1}{2}(h \mu_t(X_t) + \sigma_t(W_{t+h} - W_t))^\top \nabla^2 f(X_t)(h \mu_t(X_t) + \sigma_t(W_{t+h} - W_t)) + o(h^2) \\
                &= h \nabla f(X_t)^\top \mu_t(X_t) + \sigma_t \nabla f(X_t)^\top(W_{t+h} - W_t) \\
                &\quad + \frac{1}{2} \sigma_t^2(W_{t+h} - W_t)^\top \nabla^2 f(X_t)(W_{t+h} - W_t) + o(h^2)
                \end{aligned}$$
            </div>
            
            <p>Now take the conditional expectation w.r.t. $X_t$ to both sides. We will nullify the second term using the fact that the Wiener process is independent with $X_t$ and has zero mean. The third term on RHS will be simplified by the corollary we derived earlier, which suggests that</p>
            
            <div class="equation">
                $$\mathbb{E}[(W_{t+h} - W_t)^\top \nabla^2 u (W_{t+h} - W_t)] = \Delta u$$
            </div>
            
            <p>With these observations, we arrive at</p>
            
            <div class="equation">
                $$\mathbb{E}[f(X_{t+h}) - f(X_t) | X_t] = h \nabla f(X_t)^\top \mu_t(X_t) + \frac{h}{2} \sigma_t^2(X_t) \Delta f(X_t) + o(h^2)$$
            </div>
            
            <p>Finally, marginalize the conditional expectation by computing integrals, and call the integration by parts formula for vector fields, we have</p>
            
            <div class="equation">
                $$\begin{aligned}
                \partial_t \mathbb{E}[f(X_t)] &= \int f(x) (\partial_t p_t(x)) dx = \lim_{h\to 0} \frac{1}{h} \mathbb{E}[f(X_{t+h}) - f(X_t)] \\
                &= \lim_{h\to 0} \frac{1}{h} \mathbb{E}[\mathbb{E}[f(X_{t+h}) - f(X_t) | X_t]] \\
                &= \mathbb{E}[\nabla f(X_t)^\top \mu_t(X_t) + \frac{1}{2} \sigma_t^2(X_t) \Delta f(X_t)] \\
                &= \int \nabla f(x)^\top \mu_t(x) p_t(x) dx + \frac{1}{2} \int \Delta f(x) \cdot \sigma_t^2(x) p_t(x) dx \\
                &= -\int f(x) \text{div}(\mu_t p_t)(x) dx + \frac{1}{2} \int f(x) \Delta(\sigma_t^2 p_t)(x) dx \\
                &= \int f(x)\left[-\text{div}(\mu_t p_t)(x) + \frac{1}{2} \Delta(\sigma_t^2 p_t)(x)\right] dx
                \end{aligned}$$
            </div>
            
            <p>Now since this result holds for any test functions, at any point $x_0$ we can pick $f(x) = \delta(x-x_0)$ and obtain</p>
            
            <div class="equation">
                $$\partial_t p_t(x)\bigg|_{x=x_0} = -\text{div}(\mu_t(x) p_t(x)) + \frac{1}{2} \Delta(\sigma_t^2(x)p_t(x))\bigg|_{x=x_0}$$
            </div>
            
            <p>thus these two functionals are equivalent point-wise.</p>
        </div>
    </section>

    <section class="section" id="special-case-ode">
        <h2>3. Special Case I: ODE, Continuity Equation, and Flow-matching</h2>
        <p>With the Fokker-Planck equation, we naturally obtain an ODE counterpart by setting the noise magnitude $\sigma_t(X_t)$ to zero. This is known as the continuity equation.</p>
        
        <div class="corollary">
            <h3>Corollary (Continuity Equation)</h3>
            <p>Let $p_t$ be a probability path and consider the ODE</p>
            
            <div class="equation">
                $$X_0 \sim p_0, \quad dX_t = \mu(X_t, t) dt$$
            </div>
            
            <p>where $X_t \in \mathbb{R}^d$. Then $X_t$ has distribution $p_t$ for all $0 \leq t \leq 1$ if and only if the continuity equation holds:</p>
            
            <div class="equation">
                $$\partial_t p_t(x) = -\nabla \cdot (p_t \mu_t)(x) \quad \text{for all } x \in \mathbb{R}^d, 0 \leq t \leq 1$$
            </div>
            
            <p>where we use the abbreviations $\mu_t(x) = \mu(x_t, t)$.</p>
        </div>
        
        <p>This relationship is very useful in the derivation of flow-matching process's log probabilities.</p>
    </section>

    <section class="section" id="special-case-diffusion">
        <h2>4. Special Case II: Forward and Reverse-time Diffusion Process</h2>
        
        <div class="definition">
            <h3>Definition (Diffusion Process)</h3>
            <p>A <strong>diffusion process</strong> is a continuous-time stochastic process $\{X_t\}_{t\geq 0}$ governed by the Itô stochastic differential equation (SDE):</p>
            
            <div class="equation">
                $$dX_t = f(X_t, t) dt + g(t) dW_t$$
            </div>
            
            <p>where:</p>
            <ul>
                <li>$f(X_t, t): \mathbb{R}^d \times \mathbb{R}_+ \to \mathbb{R}^d$ is the drift coefficient,</li>
                <li>$g(t): \mathbb{R}_+ \to \mathbb{R}_+$ is the state-independent diffusion coefficient,</li>
                <li>$W_t$ is a standard $d$-dimensional Brownian motion (Wiener Process)</li>
            </ul>
            
            <p>The key characteristic of a diffusion process is that the diffusion coefficient $g(t)$ depends only on time $t$ and not on the current state $X_t$, distinguishing it from the general Itô SDE where $\sigma(X_t, t)$ may depend on the state.</p>
        </div>
        
        <p>By the Fokker-Planck equation, the marginal probability density of a forward diffusion process is given by</p>
        
        <div class="equation">
            $$\frac{\partial p_t(x)}{\partial t} = -\nabla \cdot (f(x, t) p_t(x)) + \frac{1}{2} g^2(t) \nabla^2 p_t(x)$$
        </div>
        
        <p>It turns out that the forward diffusion process is actually invertible mathematically. In detail, there exists a reverse-time diffusion process with density $\tilde{p}_s(x)$, such that $\tilde{p}_s(x) = p_{T-s}(x)$. When the reverse-time process evolves from $s=0$ to $s=T$, the marginal probability is exactly the same as the forward process evolved backward. Next, we find the SDE for the time-reversed process.</p>

        <h3>4.1 Time Reversal and Density Evolution</h3>
        <p>We denote by $s$ the increasing time index of the reverse process, and let $t$ be the increasing time index of the forward process. With a time endpoint $T$, these two time indices are related with $s+t=T$. Since $ds = -dt$, we obtain that the evolution of the reversed density with respect to the forward time $s$ is:</p>
        
        <div class="equation">
            $$\frac{\partial \tilde{p}_s(x)}{\partial s} = \frac{\partial p_{T-s}(x)}{\partial s} = -\frac{\partial p_t(x)}{\partial t}\bigg|_{t=T-s}$$
        </div>
        
        <p>Substituting the Forward Fokker-Planck Equation into the above, we can express the probability density of the reverse process in terms of the forward diffusion coefficients:</p>
        
        <div class="equation">
            $$\begin{aligned}
            \frac{\partial \tilde{p}_s(x)}{\partial s} &= -\left[-\nabla \cdot (f(x, t) p_t(x)) + \frac{1}{2} g^2(t) \nabla^2 p_t(x)\right]\bigg|_{t=T-s} \\
            &= \nabla \cdot (f(x, T-s) \tilde{p}_s(x)) - \frac{1}{2} g^2(T-s) \nabla^2 \tilde{p}_s(x)
            \end{aligned}$$
        </div>

        <h3>4.2 Diffusion, Laplacian, and Score</h3>
        <p><strong>Since the magnitude of the noise of a diffusion process is irrelevant to the state, the Laplacian in the Fokker-Planck equation will be applied directly to the probability density. By the theorem we derived earlier, this naturally results in score functions; hence in diffusion processes, the Laplacian are very closely related to the score function. This is why the score appears so many times in diffusion model learning.</strong> Concretely speaking, the Laplacian in the diffusion term can be simplified as</p>
        
        <div class="equation">
            $$\frac{1}{2} g^2(T-s) \nabla^2 \tilde{p}_s(x) = \frac{1}{2} g^2(T-s) \nabla \cdot (\tilde{p}_s(x) \nabla \log \tilde{p}_s(x))$$
        </div>
        
        <p>Substituting this into the reversed density evolution and factoring out the divergence operator and the reverse density, we have</p>
        
        <div class="equation">
            $$\frac{\partial \tilde{p}_s(x)}{\partial s} = \nabla \cdot (\tilde{p}_s(x) [f(x, T-s) - \frac{1}{2} g^2(T-s) \nabla \log \tilde{p}_s(x)])$$
        </div>

        <h3>4.3 The Reverse-Time SDE</h3>
        <p>If the reverse process exists and is also a diffusion process, it must also be governed by an FP equation with state-irrelevant noise. Let the density $\tilde{p}_s(x)$ of the hypothesized reverse-time SDE be determined by</p>
        
        <div class="equation">
            $$d\tilde{X}_s = h(\tilde{X}_s, s) ds + \tilde{g}(s) dW_s$$
        </div>
        
        <p>since it is the reverse process of $p$, it must also satisfy its own Fokker-Planck equation:</p>
        
        <div class="equation">
            $$\frac{\partial \tilde{p}_s(x)}{\partial s} = -\nabla \cdot (h(x, s) \tilde{p}_s(x)) + \frac{1}{2} \tilde{g}^2(s) \nabla^2 \tilde{p}_s(x)$$
        </div>
        
        <p><strong>If we assume that the diffusion coefficient remains the same, $\tilde{g}(s) = g(T-s)$</strong>, we can equate the two equations to determine the reverse drift $h(x, s)$.</p>
        
        <p>By factoring the reverse FP equation using the identity $\frac{1}{2} \tilde{g}^2 \nabla^2 \tilde{p}_s = -\nabla \cdot (-\frac{1}{2} \tilde{g}^2 \tilde{p}_s \nabla \log \tilde{p}_s)$:</p>
        
        <div class="equation">
            $$\frac{\partial \tilde{p}_s(x)}{\partial s} = -\nabla \cdot (\tilde{p}_s(x) [h(x, s) - \frac{1}{2} g^2(T-s) \nabla \log \tilde{p}_s(x)])$$
        </div>
        
        <p>Equating the drift terms from this and the previous equation:</p>
        
        <div class="equation">
            $$-[h(x, s) - \frac{1}{2} g^2(T-s) \nabla \log \tilde{p}_s(x)] = f(x, T-s) - \frac{1}{2} g^2(T-s) \nabla \log \tilde{p}_s(x)$$
        </div>
        
        <p>Solving for the reverse drift $h(x, s)$:</p>
        
        <div class="equation">
            $$h(x, s) = -f(x, T-s) + g^2(T-s) \nabla \log \tilde{p}_s(x)$$
        </div>
        
        <p>Substituting $h(\tilde{X}_s, s)$ back into the reverse SDE and replacing $\tilde{p}_s(\tilde{X}_s)$ with $p_{T-s}(\tilde{X}_s)$, we get the Reverse-Time Diffusion Process Equation:</p>
        
        <div class="equation">
            $$\text{Reverse diffusion SDE: } \mathrm{d}\tilde{X}_s = [-f(\tilde{X}_s, T-s) + g^2(T-s) \nabla \log p_{T-s}(\tilde{X}_s)] ds + g(T-s) dW_s, \quad s \in [0,T]$$
        </div>

        <h4>The convention of reversed time index</h4>
        <p>The equation derived above is valid for a process $\tilde{p}_s$ running in a forward-time index $s \in [0, T]$. However, literature, particularly in generative modeling, often maintains the original time index $t \in [0, T]$ but interprets the process as running backward from $T$ to $0$.</p>
        
        <p>Let us use the original time index $t$, but define the differential as a negative change $d(-t)$. Let $x(t)$ be the reverse process, where $t$ now runs from $T$ (start) to $0$ (end). The equation can be written as</p>
        
        <div class="equation">
            $$dx_t = [f(x_t, t) - g^2(t) \nabla \log p_t(x_t)] (-dt) + g(t) d\bar{W}_t$$
        </div>
        
        <p>and we remind the reader that now $t$ decreases from $T$ to $0$. Here, $(-dt)$ indicates the backward direction and $d\bar{W}_t$ is a backward Wiener process, which means that when $t$ decreases, it is statistically equivalent to $dW_s$ with $s$ increasing.</p>
        
        <p>To fully express the backward time, we abuse notations and write $-dt$ as $dt$, though it violates the convention that infinitesimal time should be positive. Then with a negative $dt$, we arrive at what is seen in the diffusion model literature:</p>
        
        <div class="equation">
            $$\text{Reverse diffusion SDE (ML convention): } dx_t = [f(x_t, t) - g^2(t) \nabla \log p_t(x_t)] dt + g(t) d\bar{W}_t, \quad t \in [T,0]$$
        </div>
        
        <p>and we remind the readers again that this $dt$ is negative in machine learning's convention.</p>
    </section>
</body>
</html>
